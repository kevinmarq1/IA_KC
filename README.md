# ğŸ§  Asistente Barman Virtual (Proyecto RAG en Local)

Este proyecto es una **implementaciÃ³n didÃ¡ctica** de un sistema RAG (*Retrieval-Augmented Generation*) que responde preguntas sobre recetas de cÃ³cteles usando como Ãºnica fuente un PDF local. La arquitectura combina un buscador semÃ¡ntico con un modelo de lenguaje, sin depender de APIs externas, ideal para pruebas educativas, experimentaciÃ³n o demostraciones.

---

## ğŸ“Œ Â¿QuÃ© es un RAG?

RAG combina dos mundos:
- **Retriever**: recupera informaciÃ³n relevante desde una base de documentos.
- **Generator**: responde preguntas en lenguaje natural usando el contexto obtenido.

En este caso, el modelo aprende a responder consultas como:
> _Â¿QuÃ© cÃ³ctel puedo hacer con ron y jugo de naranja?_

... usando exclusivamente un documento PDF con recetas de barman.

---

## ğŸ”§ TecnologÃ­as utilizadas

- `Python 3.10+`
- `LangChain` â€“ Marco modular para sistemas LLM.
- `FAISS` â€“ Motor de bÃºsqueda vectorial local.
- `Transformers` â€“ Modelo de lenguaje (Flan-T5).
- `PyMuPDF (fitz)` â€“ ExtracciÃ³n de texto del PDF.
- `HuggingFace Embeddings` â€“ VectorizaciÃ³n semÃ¡ntica.
- `gdown` â€“ Descarga directa desde Google Drive.

---

## ğŸ“¦ Requisitos e instalaciÃ³n

### 1. Clona el repositorio

```bash
git clone https://github.com/tu-usuario/barman-rag-local.git
cd barman-rag-local
```

pip install -r requirements.txt
---
ğŸš€ CÃ³mo usar
ğŸ§ª Modo notebook (interactivo)
Abre el notebook Practica_IA_KC_KevinML.ipynb (por ejemplo en Colab o Jupyter).

Ejecuta las celdas paso a paso.

Realiza preguntas en lenguaje natural como:

Â¿QuÃ© coctel puedo preparar con jugo de piÃ±a y ron?
El sistema extrae texto del PDF, lo vectoriza con FAISS, busca el contexto mÃ¡s relevante y genera una respuesta usando Flan-T5, todo en local.
---
barman-rag-local/
â”‚
â”œâ”€â”€ data/                     â† Carpeta para el PDF y texto procesado
â”œâ”€â”€ modules/                  â† CÃ³digo modular dividido en:
â”‚   â”œâ”€â”€ loader.py             â† Carga y divide el PDF en chunks
â”‚   â”œâ”€â”€ embedder.py           â† Carga modelo de embeddings
â”‚   â”œâ”€â”€ retriever.py          â† Crea y/o guarda el Ã­ndice FAISS
â”‚   â”œâ”€â”€ generator.py          â† Llama al modelo Flan-T5 con contexto
â”‚   â””â”€â”€ main.py               â† Script de prueba funcional automÃ¡tica
â”‚
â”œâ”€â”€ Practica_IA_KC_KevinML.ipynb  â† Notebook principal paso a paso
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
---
ğŸ“ Consideraciones
Este proyecto no es perfecto: el modelo puede dar respuestas incongruentes o inexactas. No estÃ¡ entrenado con recetas, solo genera texto a partir del contexto.

Sirve como base funcional y didÃ¡ctica para explorar arquitectura RAG.

Todo se ejecuta en local: sin claves API, sin costes externos, ideal para aprender sin depender de OpenAI, Anthropic o Hugging Face Spaces.

ğŸ“š CrÃ©ditos
Desarrollado como parte del mÃ³dulo de Inteligencia Artificial del bootcamp KeepCoding Big Data & Machine Learning.
Inspirado en aplicaciones reales de RAG para asistentes conversacionales.

ğŸ’¡ Ideas de mejora
Reemplazar el modelo por uno mÃ¡s pequeÃ±o o especializado (ej: bart, t5-small).

Usar mÃºltiples documentos PDF o cargar datos desde una base de datos.

Crear una interfaz web con Gradio o Streamlit.

AÃ±adir un sistema de feedback para validar respuestas.

ğŸ“œ Licencia
Este proyecto es de libre uso para fines educativos.
