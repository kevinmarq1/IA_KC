{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 Proyecto Didáctico RAG: Barman Virtual con PDF\n",
        "\n",
        "Este proyecto demuestra una arquitectura sencilla de **RAG (Retrieval-Augmented Generation)** aplicada a un escenario práctico y entretenido: un **asistente virtual para recetas de cócteles**, que responde preguntas en lenguaje natural utilizando como única fuente un documento PDF con recetas.\n",
        "\n",
        "La solución está construida con Python y librerías open-source como LangChain, FAISS, HuggingFace y Transformers. El flujo completo incluye:\n",
        "\n",
        "- 🧾 Extracción y limpieza de texto desde un PDF.\n",
        "- ✂️ Divisiones en fragmentos (chunks) para contextualizar.\n",
        "- 📚 Creación de un buscador semántico (retriever) con embeddings.\n",
        "- 🧠 Generación de respuestas contextualizadas con el modelo **Flan-T5**.\n",
        "\n",
        "Es un proyecto pensado **para aprendizaje y exploración técnica**, ideal como punto de partida para construir sistemas más complejos o adaptarlo a otros dominios (leyes, medicina, educación, etc.).\n",
        "\n",
        "> ⚙️ Todo se ejecuta **en local**, sin depender de APIs de pago, lo que permite experimentar sin restricciones ni costes adicionales.\n"
      ],
      "metadata": {
        "id": "Mu6ZDC_6Ut01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧱 Función `safe_write` para guardar módulos sin sobrescribir\n",
        "\n",
        "definimos la función `safe_write`, que permite crear archivos `.py` evitando sobrescribir archivos existentes sin confirmación. Será utilizada más adelante para generar los archivos de los módulos (`loader.py`, `retriever.py`, `generator.py`, etc.) de forma segura.\n"
      ],
      "metadata": {
        "id": "Il83DpLbHwFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ee69f89"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "def safe_write(filepath, content):\n",
        "    if os.path.exists(filepath):\n",
        "        res = input(f\"⚠️ '{filepath}' ya existe. ¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "        if res != \"s\":\n",
        "            print(f\"⏩ '{filepath}' se ha mantenido sin cambios.\")\n",
        "            return\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    print(f\"✅ Archivo '{{filepath}}' creado con éxito.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧪 Instalamos las dependencias necesarias para ejecutar el sistema de RAG\n",
        "\n",
        "En esta celda instalamos todas las librerías que usaremos en nuestro flujo de recuperación y generación de respuestas. Incluimos:\n",
        "\n",
        "- **LangChain y LangChain Community**: para la orquestación de cargado, embebido y recuperación de contexto.\n",
        "- **Sentence Transformers**: para crear los embeddings semánticos con Hugging Face.\n",
        "- **Transformers + Torch**: para ejecutar el modelo generador Flan-T5.\n",
        "- **FAISS CPU**: como motor de búsqueda vectorial para los chunks del documento.\n",
        "- **PyMuPDF y PyPDF**: para leer el PDF del recetario desde el disco.\n"
      ],
      "metadata": {
        "id": "kO2r8Q8nIldd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rzqj7TRPHm-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0f2c53-97be-4011-aa35-70e43f648624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Instalamos solo las dependencias necesarias y relevantes para el flujo funcional del proyecto\n",
        "!pip install -q langchain langchain-community sentence-transformers pymupdf pypdf faiss-cpu transformers torch accelerate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📥 Descargamos el PDF del recetario y extraemos su contenido limpio\n",
        "\n",
        "En esta celda automatizamos la descarga del documento base: el recetario de cócteles llamado **\"Guía del Barman\"**.\n",
        "\n",
        "1. Verificamos si el archivo ya existe. Si es así, preguntamos si se desea reemplazarlo.\n",
        "2. Si no existe, lo descargamos directamente desde Google Drive usando `gdown`.\n",
        "3. Abrimos el archivo PDF con `PyMuPDF` (`fitz`) y extraemos su texto página por página.\n",
        "4. Realizamos una limpieza básica del texto (eliminando espacios y líneas vacías).\n",
        "5. Finalmente, guardamos el texto limpio en un archivo `recetas.txt` dentro de la carpeta `data/`.\n",
        "\n",
        "Este paso permite tener una copia legible y estructurada del documento para inspección rápida o debug si algo falla más adelante.\n"
      ],
      "metadata": {
        "id": "gLAW0FHmKsef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN9BvrmLHoJA",
        "outputId": "e4e34c13-a79c-4936-a579-60720cee276e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Descargando PDF desde Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aFwI0tw6fClegyufHJcbHzKGmaMZq-Lz\n",
            "To: /content/data/guia_del_barman.pdf\n",
            "100%|██████████| 274k/274k [00:00<00:00, 82.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PDF descargado a data/guia_del_barman.pdf\n",
            "📖 Extrayendo texto del PDF...\n",
            "✅ Extracción completada.\n",
            "📄 Texto limpio guardado en 'data/recetas.txt'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gdown\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# 🔄 Asegurar que existe la carpeta para datos y modulos\n",
        "# 🔧 Crear carpeta modules si no existe\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"modules\", exist_ok=True)\n",
        "\n",
        "# 📦 ID del archivo en Google Drive\n",
        "file_id = \"1aFwI0tw6fClegyufHJcbHzKGmaMZq-Lz\"\n",
        "pdf_path = \"data/guia_del_barman.pdf\"\n",
        "\n",
        "# 📥 Descargar el PDF con gdown\n",
        "if os.path.exists(pdf_path):\n",
        "    respuesta = input(f\"⚠️ El archivo {pdf_path} ya existe. ¿Deseas reemplazarlo? (s/n): \").strip().lower()\n",
        "    if respuesta == 's':\n",
        "        gdown.download(id=file_id, output=pdf_path, quiet=False)\n",
        "        print(\"✅ PDF reemplazado con éxito.\")\n",
        "    else:\n",
        "        print(\"⏩ Se ha mantenido el archivo existente sin cambios.\")\n",
        "else:\n",
        "    print(\"🔄 Descargando PDF desde Google Drive...\")\n",
        "    gdown.download(id=file_id, output=pdf_path, quiet=False)\n",
        "    print(\"✅ PDF descargado a\", pdf_path)\n",
        "\n",
        "# 📄 Abrir el PDF con PyMuPDF y extraer texto completo\n",
        "print(\"📖 Extrayendo texto del PDF...\")\n",
        "doc = fitz.open(pdf_path)\n",
        "texto = \"\"\n",
        "for page in doc:\n",
        "    texto += page.get_text()\n",
        "doc.close()\n",
        "print(\"✅ Extracción completada.\")\n",
        "\n",
        "# 🧹 Limpieza mínima: eliminar líneas vacías y espacios\n",
        "texto_limpio = \"\\n\".join([line.strip() for line in texto.splitlines() if line.strip()])\n",
        "\n",
        "# 💾 Guardar en data/recetas.txt\n",
        "recetas_txt = \"data/recetas.txt\"\n",
        "with open(recetas_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(texto_limpio)\n",
        "print(f\"📄 Texto limpio guardado en '{recetas_txt}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✂️ Dividimos el texto en fragmentos manejables (chunks)\n",
        "\n",
        "En esta celda cargamos el texto limpio previamente extraído del PDF y lo dividimos en fragmentos más pequeños (chunks) para facilitar su uso posterior por el modelo.\n",
        "\n",
        "1. Cargamos el archivo `recetas.txt` desde la carpeta `data/`.\n",
        "2. Utilizamos `CharacterTextSplitter` de LangChain para dividir el texto por saltos de línea, con un tamaño máximo de 1000 caracteres por fragmento y una superposición de 100 caracteres para mantener el contexto entre bloques.\n",
        "3. Finalmente, mostramos cuántos chunks se generaron y un ejemplo de contenido para verificar el resultado.\n",
        "\n",
        "Este paso es clave para preparar el texto antes de crear embeddings o alimentar el modelo.\n"
      ],
      "metadata": {
        "id": "hNYBCTHYQSCr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7nLkIDNMXRB",
        "outputId": "6db8d537-c903-43e0-eef8-a5964a11e2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Texto dividido en 138 chunks.\n",
            "🧪 Ejemplo de chunk:\n",
            "\n",
            "Brenda\n",
            "LA GUIA\n",
            "DEL\n",
            "BARMAN\n",
            "Aperitivos\n",
            "Según el diccionario aperitivo es todo lo que despierta el apetito. Y eso es lo\n",
            "que  logran  ciertas  bebidas  que,  tomadas  antes  de  la  comida,  significan  una\n",
            "especie de rito. Sobre todo en muchos países europeos, que , entre otras cosas,\n",
            "también  producen  los  mejores  vinos  aperitivos.  Los  más  conocidos  son  el\n",
            "jerez, el madeira y el vermounth.\n",
            "Jerez\n",
            "Esta  es  una  bebida  típicamente  española.  Nació  en  Andalucía,  en  la  antigua\n",
            "ciudad de\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# 📥 Cargar el texto desde el archivo plano\n",
        "with open(\"data/recetas.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texto_limpio = f.read()\n",
        "\n",
        "# ✂️ Inicializar el splitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",       # Separador por saltos de línea\n",
        "    chunk_size=1000,      # Tamaño máximo por chunk (caracteres)\n",
        "    chunk_overlap=100,    # Superposición para contexto\n",
        ")\n",
        "\n",
        "# 🔄 Crear los chunks\n",
        "chunks = text_splitter.split_text(texto_limpio)\n",
        "\n",
        "# 📊 Mostrar resumen\n",
        "print(f\"✅ Texto dividido en {len(chunks)} chunks.\")\n",
        "print(\"🧪 Ejemplo de chunk:\\n\")\n",
        "print(chunks[0][:500])  # Mostrar solo parte del primer chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🛠️ Añadimos la carpeta `modules/` al entorno de trabajo\n",
        "\n",
        "En esta celda, nos aseguramos de que la carpeta `modules/`, donde están definidos nuestros scripts auxiliares (`loader.py`, `retriever.py`, `generator.py`, etc.), esté incluida en el path del sistema (`sys.path`). Esto es necesario para poder importarlos correctamente desde cualquier parte del notebook o del proyecto.\n",
        "\n",
        "Este paso permite mantener una estructura modular del código sin tener problemas de importación.\n"
      ],
      "metadata": {
        "id": "Gg0YV06AQiFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf_YEyI-YPHk",
        "outputId": "5e12860a-3c47-454d-af76-9982731760c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 'modules' agregado al sys.path\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# 📁 Asegura que la carpeta 'modules' esté en el path\n",
        "module_path = os.path.abspath(\"modules\")\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    print(\"📦 'modules' agregado al sys.path\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔧 Generamos el archivo `embedder.py` para cargar embeddings\n",
        "\n",
        "En esta celda definimos el módulo `embedder.py`, el cual encapsula la carga del modelo de embeddings `all-MiniLM-L6-v2` desde HuggingFace. Este modelo transforma fragmentos de texto en vectores numéricos que luego son usados por FAISS para la búsqueda semántica.\n",
        "\n",
        "Este script se guarda dentro de la carpeta `modules/`, y se utiliza en la creación del `retriever`.\n"
      ],
      "metadata": {
        "id": "ubjc3oCnQ_Qe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01iEhxXnN5AA",
        "outputId": "d5133a0d-f546-47ef-97b1-5104a8197c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'modules/embedder.py' creado con éxito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/embedder.py\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"⚠️ '{file_path}' ya existe. ¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"⏩ '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "embedder.py 📎\n",
        "Módulo encargado de cargar el modelo de embeddings 'all-MiniLM-L6-v2'\n",
        "usando HuggingFaceEmbeddings para transformar texto en vectores numéricos.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def load_embedder(model_name: str = \"all-MiniLM-L6-v2\") -> HuggingFaceEmbeddings:\n",
        "    \"\"\"\n",
        "    Carga el modelo HuggingFaceEmbeddings con el nombre especificado.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Nombre del modelo (por defecto: \"all-MiniLM-L6-v2\")\n",
        "\n",
        "    Returns:\n",
        "        HuggingFaceEmbeddings: Objeto cargado de embeddings\n",
        "    \"\"\"\n",
        "    return HuggingFaceEmbeddings(model_name=model_name)\n",
        "''')\n",
        "    print(f\"✅ Archivo '{file_path}' creado con éxito.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 Crear o cargar el sistema de recuperación de contexto (retriever)\n",
        "\n",
        "En esta celda generamos o cargamos un `retriever`, que es el sistema responsable de encontrar fragmentos relevantes del documento para responder preguntas. Utilizamos FAISS junto con embeddings del modelo `all-MiniLM-L6-v2`.\n",
        "\n",
        "- Si ya existe un índice guardado en disco (`retriever.pkl`), simplemente lo cargamos.\n",
        "- Si no existe, se debe proporcionar una lista de documentos (`docs`) para construirlo desde cero.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VhZLUUVdRfzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC3RZ50jWVf4",
        "outputId": "c4c862d4-ae5c-434e-c40a-5461677524b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'modules/retriever.py' creado o actualizado con éxito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "file_path = \"modules/retriever.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"⚠️ El archivo '{file_path}' ya existe. ¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"⏩ '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "retriever.py\n",
        "Este módulo crea o carga un retriever local usando FAISS y embeddings HuggingFace.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def create_or_load_retriever(docs=None, save_path=\"retriever/faiss_index\"):\n",
        "    \"\"\"\n",
        "    Crea o carga el retriever desde disco. Si no existe, se requiere pasar los documentos.\n",
        "    \"\"\"\n",
        "    retriever_file = os.path.join(save_path, \"retriever.pkl\")\n",
        "\n",
        "    if os.path.exists(retriever_file):\n",
        "        print(\"📦 Cargando retriever desde disco...\")\n",
        "        with open(retriever_file, \"rb\") as f:\n",
        "            retriever = pickle.load(f)\n",
        "    else:\n",
        "        if not docs:\n",
        "            raise ValueError(\"❌ No se encontraron datos para construir el retriever.\")\n",
        "        print(\"🧠 Creando nuevo retriever con embeddings...\")\n",
        "        embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        retriever = FAISS.from_documents(docs, embedding_model)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        with open(retriever_file, \"wb\") as f:\n",
        "            pickle.dump(retriever, f)\n",
        "\n",
        "    return retriever.as_retriever()\n",
        "''')\n",
        "    print(f\"✅ Archivo '{file_path}' creado o actualizado con éxito.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🤖 Generamos el archivo `generator.py` para respuestas contextuales\n",
        "\n",
        "En esta celda creamos el archivo `generator.py`, encargado de usar un modelo de lenguaje (Flan-T5) para responder preguntas del usuario. Este modelo se alimenta del contexto relevante extraído del recetario usando un retriever semántico (FAISS).\n",
        "\n",
        "El prompt guía al modelo para actuar como un barman experto, limitando sus respuestas únicamente a la información contenida en los documentos. Se utiliza `transformers` para el modelado, y el resultado es una respuesta natural y coherente basada en el texto disponible.\n"
      ],
      "metadata": {
        "id": "x2qipVfIRwRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ns9yzWo-S5Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6733212-e920-4318-d087-c3bc904e2824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'modules/generator.py' creado o actualizado con éxito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/generator.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"⚠️ El archivo '{file_path}' ya existe. ¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"⏩ '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "generator.py\n",
        "Genera respuestas con contexto usando un modelo de lenguaje (Flan-T5) y un retriever basado en FAISS.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "\n",
        "# Cargar modelo y tokenizer solo una vez\n",
        "model_name = \"google/flan-t5-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device set to use {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "def ask_with_context(query: str, retriever: BaseRetriever, max_context_length: int = 1500) -> str:\n",
        "    \"\"\"\n",
        "    Genera una respuesta a partir de una pregunta y un retriever que devuelve documentos relevantes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar documentos contextuales\n",
        "    context_docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\\\n\\\\n\".join(doc.page_content for doc in context_docs)\n",
        "\n",
        "    # Recortar contexto si es muy largo para Flan-T5\n",
        "    if len(context) > max_context_length:\n",
        "        context = context[:max_context_length]\n",
        "\n",
        "    # Prompt\n",
        "    prompt = f\\\"\\\"\\\"Eres un barman experto. Basado únicamente en el siguiente texto del recetario, responde la pregunta del usuario de forma clara y directa. Si no encuentras la información, simplemente di \"No lo sé\".\n",
        "\n",
        "Recetario:\n",
        "{context}\n",
        "\n",
        "Pregunta: {query}\n",
        "Respuesta:\\\"\\\"\\\"\n",
        "\n",
        "    # Tokenizar y generar\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=200)\n",
        "    respuesta = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return respuesta.strip()\n",
        "''')\n",
        "    print(f\"✅ Archivo '{file_path}' creado o actualizado con éxito.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📄 Creamos el archivo `loader.py` para cargar y dividir el PDF\n",
        "\n",
        "En esta celda generamos el módulo `loader.py`, que se encarga de leer el archivo PDF `guia_del_barman.pdf` y dividir su contenido en fragmentos más pequeños llamados *chunks*.\n",
        "\n",
        "Utilizamos `PyPDFLoader` para extraer las páginas del PDF y luego aplicamos `RecursiveCharacterTextSplitter`, que divide el contenido respetando la estructura del texto, ideal para modelos que necesitan contexto fragmentado. Esta función será utilizada posteriormente para alimentar el sistema de recuperación semántica.\n"
      ],
      "metadata": {
        "id": "Os9-O07TR5M3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G7ZTmRKLfxij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75b264e-0324-4a16-93fd-62298cf45fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'modules/loader.py' creado o actualizado con éxito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/loader.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"⚠️ El archivo '{file_path}' ya existe. ¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"⏩ '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "loader.py\n",
        "Carga documentos PDF y los divide en chunks usando LangChain.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def load_documents_and_chunk(ruta_pdf=\"data/guia_del_barman.pdf\"):\n",
        "    loader = PyPDFLoader(ruta_pdf)\n",
        "    pages = loader.load()\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=150\n",
        "    )\n",
        "    chunks = splitter.split_documents(pages)\n",
        "\n",
        "    return chunks\n",
        "''')\n",
        "    print(f\"✅ Archivo '{file_path}' creado o actualizado con éxito.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🤖 Creamos el archivo `main.py` para ejecutar el sistema de preguntas con contexto automáticamente\n",
        "\n",
        "Este script `main.py` es el **archivo orquestador del proyecto**, responsable de coordinar todas las piezas del sistema: carga del documento, creación del *retriever* (sistema de recuperación semántica) y generación de la respuesta por parte del modelo de lenguaje.\n",
        "\n",
        "A diferencia de una ejecución con `input()` manual, esta versión está diseñada para **ejecución automática en Google Colab**, utilizando una **pregunta predefinida** como prueba funcional. Esto permite validar rápidamente si el sistema completo está funcionando sin intervención del usuario.\n",
        "\n",
        "Los pasos son:\n",
        "1. Se carga y divide el PDF en chunks.\n",
        "2. Se crea o recupera el vector store (FAISS) con embeddings.\n",
        "3. Se lanza una pregunta fija: *“¿Qué cóctel puedo hacer con lima y ron?”*.\n",
        "4. Se imprime la respuesta generada por el modelo Flan-T5 con contexto.\n"
      ],
      "metadata": {
        "id": "XngzX5ncSOMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/main.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"⚠️ El archivo '{file_path}' ya existe. ¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"⏩ '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "main.py\n",
        "Este script orquesta la carga del documento, el retriever y el modelo para responder una pregunta fija con contexto.\n",
        "\"\"\"\n",
        "\n",
        "from loader import load_documents_and_chunk\n",
        "from retriever import create_or_load_retriever\n",
        "from generator import ask_with_context\n",
        "\n",
        "def main():\n",
        "    # Paso 1: Cargar y dividir el documento\n",
        "    print(\"📚 Cargando y dividiendo el documento...\")\n",
        "    chunks = load_documents_and_chunk()\n",
        "\n",
        "    # Paso 2: Crear o cargar el retriever (FAISS + embeddings)\n",
        "    print(\"🔎 Generando o cargando el retriever...\")\n",
        "    retriever = create_or_load_retriever(chunks)\n",
        "\n",
        "    # Paso 3: Pregunta fija para ejecución automática en Colab\n",
        "    pregunta = \"¿Qué coctel puedo hacer con lima y ron?\"\n",
        "    print(\"\\\\n❓ Pregunta:\")\n",
        "    print(pregunta)\n",
        "\n",
        "    respuesta = ask_with_context(pregunta, retriever)\n",
        "\n",
        "    # Mostrar respuesta\n",
        "    print(\"\\\\n🧠 Respuesta del modelo:\")\n",
        "    print(respuesta)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "    print(f\"✅ Archivo '{file_path}' creado o actualizado con éxito.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vco59RD99baA",
        "outputId": "a65f1ac7-4844-46e4-ec85-ce83dd0357ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'modules/main.py' creado o actualizado con éxito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ▶️ Ejecutamos el script `main.py` desde la terminal de Colab\n",
        "\n",
        "Esta línea ejecuta directamente el script `modules/main.py` usando el sistema de comandos (`!python`) de Google Colab. Al hacerlo:\n",
        "\n",
        "- Se inicia el flujo completo: carga del PDF, vectorización, generación de respuesta.\n",
        "- Se lanza automáticamente una pregunta fija (*“¿Qué cóctel puedo hacer con lima y ron?”*).\n",
        "- Se imprime la respuesta generada por el modelo Flan-T5 usando como contexto el recetario de cócteles.\n",
        "\n",
        "Este tipo de ejecución es útil para validar que todo el pipeline funciona correctamente desde un único punto.\n"
      ],
      "metadata": {
        "id": "yI0qsmSdSZPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python modules/main.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmKnKXel9rMm",
        "outputId": "b4592c40-0d67-41c6-f336-e15f5a160b82"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cpu\n",
            "tokenizer_config.json: 2.54kB [00:00, 8.69MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 1.37MB/s]\n",
            "tokenizer.json: 2.42MB [00:00, 31.7MB/s]\n",
            "special_tokens_map.json: 2.20kB [00:00, 9.10MB/s]\n",
            "config.json: 1.40kB [00:00, 7.31MB/s]\n",
            "2025-07-13 10:30:30.711323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752402631.056192    1616 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752402631.156829    1616 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-13 10:30:31.938599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 990M/990M [00:14<00:00, 68.9MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 644kB/s]\n",
            "📚 Cargando y dividiendo el documento...\n",
            "🔎 Generando o cargando el retriever...\n",
            "🧠 Creando nuevo retriever con embeddings...\n",
            "/content/modules/retriever.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "modules.json: 100% 349/349 [00:00<00:00, 1.81MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 694kB/s]\n",
            "README.md: 10.5kB [00:00, 26.9MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 317kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 4.06MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:01<00:00, 73.1MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 1.62MB/s]\n",
            "vocab.txt: 232kB [00:00, 94.6MB/s]\n",
            "tokenizer.json: 466kB [00:00, 86.7MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 707kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.13MB/s]\n",
            "\n",
            "❓ Pregunta:\n",
            "¿Qué coctel puedo hacer con lima y ron?\n",
            "/content/modules/generator.py:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  context_docs = retriever.get_relevant_documents(query)\n",
            "\n",
            "🧠 Respuesta del modelo:\n",
            "Es un barman experto. Basado nicamente en el siguiente texto de recetario, responde la pregunta del usuario clara y directa. Si no encuentras la información, simplemente di \"No lo sé\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🤖 Pregunta interactiva al modelo con recuperación contextual (RAG)\n",
        "\n",
        "En esta celda probamos el sistema completo de pregunta-respuesta. El flujo es:\n",
        "\n",
        "1. Intentamos cargar el `retriever` guardado en disco.\n",
        "2. Si no existe, cargamos y dividimos el documento automáticamente.\n",
        "3. El usuario escribe una pregunta libre.\n",
        "4. El sistema busca fragmentos relevantes del documento y genera una respuesta basada en contexto.\n",
        "\n",
        "Esta es la prueba funcional más realista del sistema RAG (retrieval-augmented generation).\n"
      ],
      "metadata": {
        "id": "-NmjCui4Sh9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/modules\")\n",
        "\n",
        "from loader import load_documents_and_chunk\n",
        "from retriever import create_or_load_retriever\n",
        "from generator import ask_with_context\n",
        "\n",
        "# 🔍 Intentar cargar retriever desde disco; si no existe, generar los documentos\n",
        "try:\n",
        "    retriever = create_or_load_retriever()\n",
        "except ValueError:\n",
        "    print(\"📚 Cargando y dividiendo el documento porque no existe retriever aún...\")\n",
        "    docs = load_documents_and_chunk()\n",
        "    retriever = create_or_load_retriever(docs)\n",
        "\n",
        "# 📝 Entrada del usuario\n",
        "pregunta = input(\"❓ Escribe tu pregunta: \").strip()\n",
        "\n",
        "# 🧠 Generar respuesta\n",
        "respuesta = ask_with_context(pregunta, retriever)\n",
        "\n",
        "# 📢 Mostrar resultado\n",
        "print(\"\\n🧠 Respuesta del modelo:\\n\")\n",
        "print(respuesta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQFNkCBPD8ys",
        "outputId": "9b8d2659-c32d-495e-83de-8497eefb8b5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Cargando retriever desde disco...\n",
            "❓ Escribe tu pregunta: que es mojito\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/modules/generator.py:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  context_docs = retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Respuesta del modelo:\n",
            "\n",
            "El primero tiene un color claro y un aroma muy peso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Conclusión del ejercicio\n",
        "\n",
        "Este sistema demuestra con éxito una arquitectura básica de recuperación aumentada por generación (RAG), donde un modelo de lenguaje es asistido por un sistema de recuperación de contexto para responder preguntas basadas en un documento PDF.\n",
        "\n",
        "Aunque el modelo no siempre ofrece respuestas perfectamente coherentes o precisas —especialmente en recetas compuestas o mal estructuradas dentro del PDF—, **cumple con su objetivo didáctico**: demostrar la conexión funcional entre los módulos de carga, embeddings, búsqueda de contexto y generación de texto.\n",
        "\n",
        "🧪 En resumen:\n",
        "\n",
        "- El sistema es **funcional, modular y educativo**.\n",
        "- Permite experimentar con nuevas preguntas y documentos.\n",
        "- Sirve como base sólida para mejoras posteriores, como:\n",
        "  - Filtrado semántico del contexto.\n",
        "  - Fine-tuning del modelo generador.\n",
        "  - Añadir validaciones automáticas sobre la respuesta.\n",
        "\n",
        "⚙️ Además, se ha diseñado **para ejecutarse completamente en local**, lo cual evita el uso de endpoints externos como OpenAI, Google Vertex AI o Anthropic, que pueden requerir:\n",
        "\n",
        "- API Keys.\n",
        "- Registros adicionales.\n",
        "- Costes por token o uso prolongado.\n",
        "\n",
        "Este enfoque **sin fricciones y sin gastos adicionales** permite explorar conceptos avanzados de IA generativa con total autonomía, facilitando el aprendizaje y la construcción de prototipos reales sin barreras de acceso.\n",
        "\n"
      ],
      "metadata": {
        "id": "kHZQLvHMUd2H"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}