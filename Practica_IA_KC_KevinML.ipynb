{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§ª Proyecto DidÃ¡ctico RAG: Barman Virtual con PDF\n",
        "\n",
        "Este proyecto demuestra una arquitectura sencilla de **RAG (Retrieval-Augmented Generation)** aplicada a un escenario prÃ¡ctico y entretenido: un **asistente virtual para recetas de cÃ³cteles**, que responde preguntas en lenguaje natural utilizando como Ãºnica fuente un documento PDF con recetas.\n",
        "\n",
        "La soluciÃ³n estÃ¡ construida con Python y librerÃ­as open-source como LangChain, FAISS, HuggingFace y Transformers. El flujo completo incluye:\n",
        "\n",
        "- ğŸ§¾ ExtracciÃ³n y limpieza de texto desde un PDF.\n",
        "- âœ‚ï¸ Divisiones en fragmentos (chunks) para contextualizar.\n",
        "- ğŸ“š CreaciÃ³n de un buscador semÃ¡ntico (retriever) con embeddings.\n",
        "- ğŸ§  GeneraciÃ³n de respuestas contextualizadas con el modelo **Flan-T5**.\n",
        "\n",
        "Es un proyecto pensado **para aprendizaje y exploraciÃ³n tÃ©cnica**, ideal como punto de partida para construir sistemas mÃ¡s complejos o adaptarlo a otros dominios (leyes, medicina, educaciÃ³n, etc.).\n",
        "\n",
        "> âš™ï¸ Todo se ejecuta **en local**, sin depender de APIs de pago, lo que permite experimentar sin restricciones ni costes adicionales.\n"
      ],
      "metadata": {
        "id": "Mu6ZDC_6Ut01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ§± FunciÃ³n `safe_write` para guardar mÃ³dulos sin sobrescribir\n",
        "\n",
        "definimos la funciÃ³n `safe_write`, que permite crear archivos `.py` evitando sobrescribir archivos existentes sin confirmaciÃ³n. SerÃ¡ utilizada mÃ¡s adelante para generar los archivos de los mÃ³dulos (`loader.py`, `retriever.py`, `generator.py`, etc.) de forma segura.\n"
      ],
      "metadata": {
        "id": "Il83DpLbHwFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ee69f89"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "def safe_write(filepath, content):\n",
        "    if os.path.exists(filepath):\n",
        "        res = input(f\"âš ï¸ '{filepath}' ya existe. Â¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "        if res != \"s\":\n",
        "            print(f\"â© '{filepath}' se ha mantenido sin cambios.\")\n",
        "            return\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    print(f\"âœ… Archivo '{{filepath}}' creado con Ã©xito.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ§ª Instalamos las dependencias necesarias para ejecutar el sistema de RAG\n",
        "\n",
        "En esta celda instalamos todas las librerÃ­as que usaremos en nuestro flujo de recuperaciÃ³n y generaciÃ³n de respuestas. Incluimos:\n",
        "\n",
        "- **LangChain y LangChain Community**: para la orquestaciÃ³n de cargado, embebido y recuperaciÃ³n de contexto.\n",
        "- **Sentence Transformers**: para crear los embeddings semÃ¡nticos con Hugging Face.\n",
        "- **Transformers + Torch**: para ejecutar el modelo generador Flan-T5.\n",
        "- **FAISS CPU**: como motor de bÃºsqueda vectorial para los chunks del documento.\n",
        "- **PyMuPDF y PyPDF**: para leer el PDF del recetario desde el disco.\n"
      ],
      "metadata": {
        "id": "kO2r8Q8nIldd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rzqj7TRPHm-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0f2c53-97be-4011-aa35-70e43f648624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Instalamos solo las dependencias necesarias y relevantes para el flujo funcional del proyecto\n",
        "!pip install -q langchain langchain-community sentence-transformers pymupdf pypdf faiss-cpu transformers torch accelerate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“¥ Descargamos el PDF del recetario y extraemos su contenido limpio\n",
        "\n",
        "En esta celda automatizamos la descarga del documento base: el recetario de cÃ³cteles llamado **\"GuÃ­a del Barman\"**.\n",
        "\n",
        "1. Verificamos si el archivo ya existe. Si es asÃ­, preguntamos si se desea reemplazarlo.\n",
        "2. Si no existe, lo descargamos directamente desde Google Drive usando `gdown`.\n",
        "3. Abrimos el archivo PDF con `PyMuPDF` (`fitz`) y extraemos su texto pÃ¡gina por pÃ¡gina.\n",
        "4. Realizamos una limpieza bÃ¡sica del texto (eliminando espacios y lÃ­neas vacÃ­as).\n",
        "5. Finalmente, guardamos el texto limpio en un archivo `recetas.txt` dentro de la carpeta `data/`.\n",
        "\n",
        "Este paso permite tener una copia legible y estructurada del documento para inspecciÃ³n rÃ¡pida o debug si algo falla mÃ¡s adelante.\n"
      ],
      "metadata": {
        "id": "gLAW0FHmKsef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN9BvrmLHoJA",
        "outputId": "e4e34c13-a79c-4936-a579-60720cee276e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Descargando PDF desde Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aFwI0tw6fClegyufHJcbHzKGmaMZq-Lz\n",
            "To: /content/data/guia_del_barman.pdf\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 274k/274k [00:00<00:00, 82.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… PDF descargado a data/guia_del_barman.pdf\n",
            "ğŸ“– Extrayendo texto del PDF...\n",
            "âœ… ExtracciÃ³n completada.\n",
            "ğŸ“„ Texto limpio guardado en 'data/recetas.txt'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gdown\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# ğŸ”„ Asegurar que existe la carpeta para datos y modulos\n",
        "# ğŸ”§ Crear carpeta modules si no existe\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"modules\", exist_ok=True)\n",
        "\n",
        "# ğŸ“¦ ID del archivo en Google Drive\n",
        "file_id = \"1aFwI0tw6fClegyufHJcbHzKGmaMZq-Lz\"\n",
        "pdf_path = \"data/guia_del_barman.pdf\"\n",
        "\n",
        "# ğŸ“¥ Descargar el PDF con gdown\n",
        "if os.path.exists(pdf_path):\n",
        "    respuesta = input(f\"âš ï¸ El archivo {pdf_path} ya existe. Â¿Deseas reemplazarlo? (s/n): \").strip().lower()\n",
        "    if respuesta == 's':\n",
        "        gdown.download(id=file_id, output=pdf_path, quiet=False)\n",
        "        print(\"âœ… PDF reemplazado con Ã©xito.\")\n",
        "    else:\n",
        "        print(\"â© Se ha mantenido el archivo existente sin cambios.\")\n",
        "else:\n",
        "    print(\"ğŸ”„ Descargando PDF desde Google Drive...\")\n",
        "    gdown.download(id=file_id, output=pdf_path, quiet=False)\n",
        "    print(\"âœ… PDF descargado a\", pdf_path)\n",
        "\n",
        "# ğŸ“„ Abrir el PDF con PyMuPDF y extraer texto completo\n",
        "print(\"ğŸ“– Extrayendo texto del PDF...\")\n",
        "doc = fitz.open(pdf_path)\n",
        "texto = \"\"\n",
        "for page in doc:\n",
        "    texto += page.get_text()\n",
        "doc.close()\n",
        "print(\"âœ… ExtracciÃ³n completada.\")\n",
        "\n",
        "# ğŸ§¹ Limpieza mÃ­nima: eliminar lÃ­neas vacÃ­as y espacios\n",
        "texto_limpio = \"\\n\".join([line.strip() for line in texto.splitlines() if line.strip()])\n",
        "\n",
        "# ğŸ’¾ Guardar en data/recetas.txt\n",
        "recetas_txt = \"data/recetas.txt\"\n",
        "with open(recetas_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(texto_limpio)\n",
        "print(f\"ğŸ“„ Texto limpio guardado en '{recetas_txt}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ‚ï¸ Dividimos el texto en fragmentos manejables (chunks)\n",
        "\n",
        "En esta celda cargamos el texto limpio previamente extraÃ­do del PDF y lo dividimos en fragmentos mÃ¡s pequeÃ±os (chunks) para facilitar su uso posterior por el modelo.\n",
        "\n",
        "1. Cargamos el archivo `recetas.txt` desde la carpeta `data/`.\n",
        "2. Utilizamos `CharacterTextSplitter` de LangChain para dividir el texto por saltos de lÃ­nea, con un tamaÃ±o mÃ¡ximo de 1000 caracteres por fragmento y una superposiciÃ³n de 100 caracteres para mantener el contexto entre bloques.\n",
        "3. Finalmente, mostramos cuÃ¡ntos chunks se generaron y un ejemplo de contenido para verificar el resultado.\n",
        "\n",
        "Este paso es clave para preparar el texto antes de crear embeddings o alimentar el modelo.\n"
      ],
      "metadata": {
        "id": "hNYBCTHYQSCr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7nLkIDNMXRB",
        "outputId": "6db8d537-c903-43e0-eef8-a5964a11e2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Texto dividido en 138 chunks.\n",
            "ğŸ§ª Ejemplo de chunk:\n",
            "\n",
            "Brenda\n",
            "LA GUIA\n",
            "DEL\n",
            "BARMAN\n",
            "Aperitivos\n",
            "SegÃºn el diccionario aperitivo es todo lo que despierta el apetito. Y eso es lo\n",
            "que  logran  ciertas  bebidas  que,  tomadas  antes  de  la  comida,  significan  una\n",
            "especie de rito. Sobre todo en muchos paÃ­ses europeos, que , entre otras cosas,\n",
            "tambiÃ©n  producen  los  mejores  vinos  aperitivos.  Los  mÃ¡s  conocidos  son  el\n",
            "jerez, el madeira y el vermounth.\n",
            "Jerez\n",
            "Esta  es  una  bebida  tÃ­picamente  espaÃ±ola.  NaciÃ³  en  AndalucÃ­a,  en  la  antigua\n",
            "ciudad de\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# ğŸ“¥ Cargar el texto desde el archivo plano\n",
        "with open(\"data/recetas.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texto_limpio = f.read()\n",
        "\n",
        "# âœ‚ï¸ Inicializar el splitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",       # Separador por saltos de lÃ­nea\n",
        "    chunk_size=1000,      # TamaÃ±o mÃ¡ximo por chunk (caracteres)\n",
        "    chunk_overlap=100,    # SuperposiciÃ³n para contexto\n",
        ")\n",
        "\n",
        "# ğŸ”„ Crear los chunks\n",
        "chunks = text_splitter.split_text(texto_limpio)\n",
        "\n",
        "# ğŸ“Š Mostrar resumen\n",
        "print(f\"âœ… Texto dividido en {len(chunks)} chunks.\")\n",
        "print(\"ğŸ§ª Ejemplo de chunk:\\n\")\n",
        "print(chunks[0][:500])  # Mostrar solo parte del primer chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ› ï¸ AÃ±adimos la carpeta `modules/` al entorno de trabajo\n",
        "\n",
        "En esta celda, nos aseguramos de que la carpeta `modules/`, donde estÃ¡n definidos nuestros scripts auxiliares (`loader.py`, `retriever.py`, `generator.py`, etc.), estÃ© incluida en el path del sistema (`sys.path`). Esto es necesario para poder importarlos correctamente desde cualquier parte del notebook o del proyecto.\n",
        "\n",
        "Este paso permite mantener una estructura modular del cÃ³digo sin tener problemas de importaciÃ³n.\n"
      ],
      "metadata": {
        "id": "Gg0YV06AQiFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf_YEyI-YPHk",
        "outputId": "5e12860a-3c47-454d-af76-9982731760c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ 'modules' agregado al sys.path\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# ğŸ“ Asegura que la carpeta 'modules' estÃ© en el path\n",
        "module_path = os.path.abspath(\"modules\")\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    print(\"ğŸ“¦ 'modules' agregado al sys.path\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ”§ Generamos el archivo `embedder.py` para cargar embeddings\n",
        "\n",
        "En esta celda definimos el mÃ³dulo `embedder.py`, el cual encapsula la carga del modelo de embeddings `all-MiniLM-L6-v2` desde HuggingFace. Este modelo transforma fragmentos de texto en vectores numÃ©ricos que luego son usados por FAISS para la bÃºsqueda semÃ¡ntica.\n",
        "\n",
        "Este script se guarda dentro de la carpeta `modules/`, y se utiliza en la creaciÃ³n del `retriever`.\n"
      ],
      "metadata": {
        "id": "ubjc3oCnQ_Qe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01iEhxXnN5AA",
        "outputId": "d5133a0d-f546-47ef-97b1-5104a8197c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Archivo 'modules/embedder.py' creado con Ã©xito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/embedder.py\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"âš ï¸ '{file_path}' ya existe. Â¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"â© '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "embedder.py ğŸ“\n",
        "MÃ³dulo encargado de cargar el modelo de embeddings 'all-MiniLM-L6-v2'\n",
        "usando HuggingFaceEmbeddings para transformar texto en vectores numÃ©ricos.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def load_embedder(model_name: str = \"all-MiniLM-L6-v2\") -> HuggingFaceEmbeddings:\n",
        "    \"\"\"\n",
        "    Carga el modelo HuggingFaceEmbeddings con el nombre especificado.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Nombre del modelo (por defecto: \"all-MiniLM-L6-v2\")\n",
        "\n",
        "    Returns:\n",
        "        HuggingFaceEmbeddings: Objeto cargado de embeddings\n",
        "    \"\"\"\n",
        "    return HuggingFaceEmbeddings(model_name=model_name)\n",
        "''')\n",
        "    print(f\"âœ… Archivo '{file_path}' creado con Ã©xito.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ§  Crear o cargar el sistema de recuperaciÃ³n de contexto (retriever)\n",
        "\n",
        "En esta celda generamos o cargamos un `retriever`, que es el sistema responsable de encontrar fragmentos relevantes del documento para responder preguntas. Utilizamos FAISS junto con embeddings del modelo `all-MiniLM-L6-v2`.\n",
        "\n",
        "- Si ya existe un Ã­ndice guardado en disco (`retriever.pkl`), simplemente lo cargamos.\n",
        "- Si no existe, se debe proporcionar una lista de documentos (`docs`) para construirlo desde cero.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VhZLUUVdRfzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC3RZ50jWVf4",
        "outputId": "c4c862d4-ae5c-434e-c40a-5461677524b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Archivo 'modules/retriever.py' creado o actualizado con Ã©xito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "file_path = \"modules/retriever.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"âš ï¸ El archivo '{file_path}' ya existe. Â¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"â© '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "retriever.py\n",
        "Este mÃ³dulo crea o carga un retriever local usando FAISS y embeddings HuggingFace.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def create_or_load_retriever(docs=None, save_path=\"retriever/faiss_index\"):\n",
        "    \"\"\"\n",
        "    Crea o carga el retriever desde disco. Si no existe, se requiere pasar los documentos.\n",
        "    \"\"\"\n",
        "    retriever_file = os.path.join(save_path, \"retriever.pkl\")\n",
        "\n",
        "    if os.path.exists(retriever_file):\n",
        "        print(\"ğŸ“¦ Cargando retriever desde disco...\")\n",
        "        with open(retriever_file, \"rb\") as f:\n",
        "            retriever = pickle.load(f)\n",
        "    else:\n",
        "        if not docs:\n",
        "            raise ValueError(\"âŒ No se encontraron datos para construir el retriever.\")\n",
        "        print(\"ğŸ§  Creando nuevo retriever con embeddings...\")\n",
        "        embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        retriever = FAISS.from_documents(docs, embedding_model)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        with open(retriever_file, \"wb\") as f:\n",
        "            pickle.dump(retriever, f)\n",
        "\n",
        "    return retriever.as_retriever()\n",
        "''')\n",
        "    print(f\"âœ… Archivo '{file_path}' creado o actualizado con Ã©xito.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ¤– Generamos el archivo `generator.py` para respuestas contextuales\n",
        "\n",
        "En esta celda creamos el archivo `generator.py`, encargado de usar un modelo de lenguaje (Flan-T5) para responder preguntas del usuario. Este modelo se alimenta del contexto relevante extraÃ­do del recetario usando un retriever semÃ¡ntico (FAISS).\n",
        "\n",
        "El prompt guÃ­a al modelo para actuar como un barman experto, limitando sus respuestas Ãºnicamente a la informaciÃ³n contenida en los documentos. Se utiliza `transformers` para el modelado, y el resultado es una respuesta natural y coherente basada en el texto disponible.\n"
      ],
      "metadata": {
        "id": "x2qipVfIRwRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ns9yzWo-S5Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6733212-e920-4318-d087-c3bc904e2824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Archivo 'modules/generator.py' creado o actualizado con Ã©xito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/generator.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"âš ï¸ El archivo '{file_path}' ya existe. Â¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"â© '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "generator.py\n",
        "Genera respuestas con contexto usando un modelo de lenguaje (Flan-T5) y un retriever basado en FAISS.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "\n",
        "# Cargar modelo y tokenizer solo una vez\n",
        "model_name = \"google/flan-t5-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device set to use {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "def ask_with_context(query: str, retriever: BaseRetriever, max_context_length: int = 1500) -> str:\n",
        "    \"\"\"\n",
        "    Genera una respuesta a partir de una pregunta y un retriever que devuelve documentos relevantes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar documentos contextuales\n",
        "    context_docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\\\n\\\\n\".join(doc.page_content for doc in context_docs)\n",
        "\n",
        "    # Recortar contexto si es muy largo para Flan-T5\n",
        "    if len(context) > max_context_length:\n",
        "        context = context[:max_context_length]\n",
        "\n",
        "    # Prompt\n",
        "    prompt = f\\\"\\\"\\\"Eres un barman experto. Basado Ãºnicamente en el siguiente texto del recetario, responde la pregunta del usuario de forma clara y directa. Si no encuentras la informaciÃ³n, simplemente di \"No lo sÃ©\".\n",
        "\n",
        "Recetario:\n",
        "{context}\n",
        "\n",
        "Pregunta: {query}\n",
        "Respuesta:\\\"\\\"\\\"\n",
        "\n",
        "    # Tokenizar y generar\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=200)\n",
        "    respuesta = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return respuesta.strip()\n",
        "''')\n",
        "    print(f\"âœ… Archivo '{file_path}' creado o actualizado con Ã©xito.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“„ Creamos el archivo `loader.py` para cargar y dividir el PDF\n",
        "\n",
        "En esta celda generamos el mÃ³dulo `loader.py`, que se encarga de leer el archivo PDF `guia_del_barman.pdf` y dividir su contenido en fragmentos mÃ¡s pequeÃ±os llamados *chunks*.\n",
        "\n",
        "Utilizamos `PyPDFLoader` para extraer las pÃ¡ginas del PDF y luego aplicamos `RecursiveCharacterTextSplitter`, que divide el contenido respetando la estructura del texto, ideal para modelos que necesitan contexto fragmentado. Esta funciÃ³n serÃ¡ utilizada posteriormente para alimentar el sistema de recuperaciÃ³n semÃ¡ntica.\n"
      ],
      "metadata": {
        "id": "Os9-O07TR5M3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G7ZTmRKLfxij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75b264e-0324-4a16-93fd-62298cf45fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Archivo 'modules/loader.py' creado o actualizado con Ã©xito.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/loader.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"âš ï¸ El archivo '{file_path}' ya existe. Â¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"â© '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "loader.py\n",
        "Carga documentos PDF y los divide en chunks usando LangChain.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def load_documents_and_chunk(ruta_pdf=\"data/guia_del_barman.pdf\"):\n",
        "    loader = PyPDFLoader(ruta_pdf)\n",
        "    pages = loader.load()\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=150\n",
        "    )\n",
        "    chunks = splitter.split_documents(pages)\n",
        "\n",
        "    return chunks\n",
        "''')\n",
        "    print(f\"âœ… Archivo '{file_path}' creado o actualizado con Ã©xito.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ¤– Creamos el archivo `main.py` para ejecutar el sistema de preguntas con contexto automÃ¡ticamente\n",
        "\n",
        "Este script `main.py` es el **archivo orquestador del proyecto**, responsable de coordinar todas las piezas del sistema: carga del documento, creaciÃ³n del *retriever* (sistema de recuperaciÃ³n semÃ¡ntica) y generaciÃ³n de la respuesta por parte del modelo de lenguaje.\n",
        "\n",
        "A diferencia de una ejecuciÃ³n con `input()` manual, esta versiÃ³n estÃ¡ diseÃ±ada para **ejecuciÃ³n automÃ¡tica en Google Colab**, utilizando una **pregunta predefinida** como prueba funcional. Esto permite validar rÃ¡pidamente si el sistema completo estÃ¡ funcionando sin intervenciÃ³n del usuario.\n",
        "\n",
        "Los pasos son:\n",
        "1. Se carga y divide el PDF en chunks.\n",
        "2. Se crea o recupera el vector store (FAISS) con embeddings.\n",
        "3. Se lanza una pregunta fija: *â€œÂ¿QuÃ© cÃ³ctel puedo hacer con lima y ron?â€*.\n",
        "4. Se imprime la respuesta generada por el modelo Flan-T5 con contexto.\n"
      ],
      "metadata": {
        "id": "XngzX5ncSOMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"modules/main.py\"\n",
        "\n",
        "# Verificar si existe y preguntar si se sobrescribe\n",
        "if os.path.exists(file_path):\n",
        "    res = input(f\"âš ï¸ El archivo '{file_path}' ya existe. Â¿Deseas reescribirlo? (s/n): \").strip().lower()\n",
        "    if res != \"s\":\n",
        "        print(f\"â© '{file_path}' se ha mantenido sin cambios.\")\n",
        "    else:\n",
        "        os.remove(file_path)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write('''\"\"\"\n",
        "main.py\n",
        "Este script orquesta la carga del documento, el retriever y el modelo para responder una pregunta fija con contexto.\n",
        "\"\"\"\n",
        "\n",
        "from loader import load_documents_and_chunk\n",
        "from retriever import create_or_load_retriever\n",
        "from generator import ask_with_context\n",
        "\n",
        "def main():\n",
        "    # Paso 1: Cargar y dividir el documento\n",
        "    print(\"ğŸ“š Cargando y dividiendo el documento...\")\n",
        "    chunks = load_documents_and_chunk()\n",
        "\n",
        "    # Paso 2: Crear o cargar el retriever (FAISS + embeddings)\n",
        "    print(\"ğŸ” Generando o cargando el retriever...\")\n",
        "    retriever = create_or_load_retriever(chunks)\n",
        "\n",
        "    # Paso 3: Pregunta fija para ejecuciÃ³n automÃ¡tica en Colab\n",
        "    pregunta = \"Â¿QuÃ© coctel puedo hacer con lima y ron?\"\n",
        "    print(\"\\\\nâ“ Pregunta:\")\n",
        "    print(pregunta)\n",
        "\n",
        "    respuesta = ask_with_context(pregunta, retriever)\n",
        "\n",
        "    # Mostrar respuesta\n",
        "    print(\"\\\\nğŸ§  Respuesta del modelo:\")\n",
        "    print(respuesta)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "    print(f\"âœ… Archivo '{file_path}' creado o actualizado con Ã©xito.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vco59RD99baA",
        "outputId": "a65f1ac7-4844-46e4-ec85-ce83dd0357ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Archivo 'modules/main.py' creado o actualizado con Ã©xito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### â–¶ï¸ Ejecutamos el script `main.py` desde la terminal de Colab\n",
        "\n",
        "Esta lÃ­nea ejecuta directamente el script `modules/main.py` usando el sistema de comandos (`!python`) de Google Colab. Al hacerlo:\n",
        "\n",
        "- Se inicia el flujo completo: carga del PDF, vectorizaciÃ³n, generaciÃ³n de respuesta.\n",
        "- Se lanza automÃ¡ticamente una pregunta fija (*â€œÂ¿QuÃ© cÃ³ctel puedo hacer con lima y ron?â€*).\n",
        "- Se imprime la respuesta generada por el modelo Flan-T5 usando como contexto el recetario de cÃ³cteles.\n",
        "\n",
        "Este tipo de ejecuciÃ³n es Ãºtil para validar que todo el pipeline funciona correctamente desde un Ãºnico punto.\n"
      ],
      "metadata": {
        "id": "yI0qsmSdSZPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python modules/main.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmKnKXel9rMm",
        "outputId": "b4592c40-0d67-41c6-f336-e15f5a160b82"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cpu\n",
            "tokenizer_config.json: 2.54kB [00:00, 8.69MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 1.37MB/s]\n",
            "tokenizer.json: 2.42MB [00:00, 31.7MB/s]\n",
            "special_tokens_map.json: 2.20kB [00:00, 9.10MB/s]\n",
            "config.json: 1.40kB [00:00, 7.31MB/s]\n",
            "2025-07-13 10:30:30.711323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752402631.056192    1616 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752402631.156829    1616 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-13 10:30:31.938599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 990M/990M [00:14<00:00, 68.9MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 644kB/s]\n",
            "ğŸ“š Cargando y dividiendo el documento...\n",
            "ğŸ” Generando o cargando el retriever...\n",
            "ğŸ§  Creando nuevo retriever con embeddings...\n",
            "/content/modules/retriever.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "modules.json: 100% 349/349 [00:00<00:00, 1.81MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 694kB/s]\n",
            "README.md: 10.5kB [00:00, 26.9MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 317kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 4.06MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:01<00:00, 73.1MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 1.62MB/s]\n",
            "vocab.txt: 232kB [00:00, 94.6MB/s]\n",
            "tokenizer.json: 466kB [00:00, 86.7MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 707kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.13MB/s]\n",
            "\n",
            "â“ Pregunta:\n",
            "Â¿QuÃ© coctel puedo hacer con lima y ron?\n",
            "/content/modules/generator.py:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  context_docs = retriever.get_relevant_documents(query)\n",
            "\n",
            "ğŸ§  Respuesta del modelo:\n",
            "Es un barman experto. Basado nicamente en el siguiente texto de recetario, responde la pregunta del usuario clara y directa. Si no encuentras la informaciÃ³n, simplemente di \"No lo sÃ©\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ¤– Pregunta interactiva al modelo con recuperaciÃ³n contextual (RAG)\n",
        "\n",
        "En esta celda probamos el sistema completo de pregunta-respuesta. El flujo es:\n",
        "\n",
        "1. Intentamos cargar el `retriever` guardado en disco.\n",
        "2. Si no existe, cargamos y dividimos el documento automÃ¡ticamente.\n",
        "3. El usuario escribe una pregunta libre.\n",
        "4. El sistema busca fragmentos relevantes del documento y genera una respuesta basada en contexto.\n",
        "\n",
        "Esta es la prueba funcional mÃ¡s realista del sistema RAG (retrieval-augmented generation).\n"
      ],
      "metadata": {
        "id": "-NmjCui4Sh9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/modules\")\n",
        "\n",
        "from loader import load_documents_and_chunk\n",
        "from retriever import create_or_load_retriever\n",
        "from generator import ask_with_context\n",
        "\n",
        "# ğŸ” Intentar cargar retriever desde disco; si no existe, generar los documentos\n",
        "try:\n",
        "    retriever = create_or_load_retriever()\n",
        "except ValueError:\n",
        "    print(\"ğŸ“š Cargando y dividiendo el documento porque no existe retriever aÃºn...\")\n",
        "    docs = load_documents_and_chunk()\n",
        "    retriever = create_or_load_retriever(docs)\n",
        "\n",
        "# ğŸ“ Entrada del usuario\n",
        "pregunta = input(\"â“ Escribe tu pregunta: \").strip()\n",
        "\n",
        "# ğŸ§  Generar respuesta\n",
        "respuesta = ask_with_context(pregunta, retriever)\n",
        "\n",
        "# ğŸ“¢ Mostrar resultado\n",
        "print(\"\\nğŸ§  Respuesta del modelo:\\n\")\n",
        "print(respuesta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQFNkCBPD8ys",
        "outputId": "9b8d2659-c32d-495e-83de-8497eefb8b5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ Cargando retriever desde disco...\n",
            "â“ Escribe tu pregunta: que es mojito\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/modules/generator.py:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  context_docs = retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§  Respuesta del modelo:\n",
            "\n",
            "El primero tiene un color claro y un aroma muy peso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ConclusiÃ³n del ejercicio\n",
        "\n",
        "Este sistema demuestra con Ã©xito una arquitectura bÃ¡sica de recuperaciÃ³n aumentada por generaciÃ³n (RAG), donde un modelo de lenguaje es asistido por un sistema de recuperaciÃ³n de contexto para responder preguntas basadas en un documento PDF.\n",
        "\n",
        "Aunque el modelo no siempre ofrece respuestas perfectamente coherentes o precisas â€”especialmente en recetas compuestas o mal estructuradas dentro del PDFâ€”, **cumple con su objetivo didÃ¡ctico**: demostrar la conexiÃ³n funcional entre los mÃ³dulos de carga, embeddings, bÃºsqueda de contexto y generaciÃ³n de texto.\n",
        "\n",
        "ğŸ§ª En resumen:\n",
        "\n",
        "- El sistema es **funcional, modular y educativo**.\n",
        "- Permite experimentar con nuevas preguntas y documentos.\n",
        "- Sirve como base sÃ³lida para mejoras posteriores, como:\n",
        "  - Filtrado semÃ¡ntico del contexto.\n",
        "  - Fine-tuning del modelo generador.\n",
        "  - AÃ±adir validaciones automÃ¡ticas sobre la respuesta.\n",
        "\n",
        "âš™ï¸ AdemÃ¡s, se ha diseÃ±ado **para ejecutarse completamente en local**, lo cual evita el uso de endpoints externos como OpenAI, Google Vertex AI o Anthropic, que pueden requerir:\n",
        "\n",
        "- API Keys.\n",
        "- Registros adicionales.\n",
        "- Costes por token o uso prolongado.\n",
        "\n",
        "Este enfoque **sin fricciones y sin gastos adicionales** permite explorar conceptos avanzados de IA generativa con total autonomÃ­a, facilitando el aprendizaje y la construcciÃ³n de prototipos reales sin barreras de acceso.\n",
        "\n"
      ],
      "metadata": {
        "id": "kHZQLvHMUd2H"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}